{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29517da2-e624-4baf-b63a-e486d0d5291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torch.nn as nn\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import multiprocessing\n",
    "\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec38bc73-0d43-4480-82e4-a888267661d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed_value=4995):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d66c96-24b9-4179-aa36-29dc6b6a3c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_frame(raw_data, image_path):\n",
    "\n",
    "    data = {}\n",
    "    data['latex'] = []\n",
    "    data['seq_len'] = []\n",
    "    data['latex_string'] = []\n",
    "    data['visible_latex_chars'] = []\n",
    "    data['filename'] = []\n",
    "    data['width'] = []\n",
    "    data['height'] = []\n",
    "    data['xmins_raw'] = []\n",
    "    data['xmaxs_raw'] = []\n",
    "    data['ymins_raw'] = []\n",
    "    data['ymaxs_raw'] = []\n",
    "    data['xmins'] = []\n",
    "    data['xmaxs'] = []\n",
    "    data['ymins'] = []\n",
    "    data['ymaxs'] = []\n",
    "    \n",
    "    for image in raw_data:\n",
    "        data['latex_string'].append(image['latex'])\n",
    "        data['latex'].append(image['image_data']['full_latex_chars'])\n",
    "        data['seq_len'].append(len(image['image_data']['full_latex_chars']))\n",
    "        data['visible_latex_chars'].append(image['image_data']['visible_latex_chars'])\n",
    "        data['filename'].append(os.path.join(image_path, image['filename']))\n",
    "        data['xmins_raw'].append(image['image_data']['xmins_raw'])\n",
    "        data['xmaxs_raw'].append(image['image_data']['xmaxs_raw'])\n",
    "        data['ymins_raw'].append(image['image_data']['ymins_raw'])\n",
    "        data['ymaxs_raw'].append(image['image_data']['ymaxs_raw'])\n",
    "        data['xmins'].append(image['image_data']['xmins'])\n",
    "        data['xmaxs'].append(image['image_data']['xmaxs'])\n",
    "        data['ymins'].append(image['image_data']['ymins'])\n",
    "        data['ymaxs'].append(image['image_data']['ymaxs'])\n",
    "        \n",
    "        data['width'].append(image['image_data']['width'])\n",
    "        data['height'].append(image['image_data']['height'])\n",
    "\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dab5763-2be4-4120-832b-c733251ea5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path = 'data/all_data.csv'):\n",
    "    if not os.path.isfile(path):\n",
    "        df = pd.DataFrame()\n",
    "        for i in range(1,11):\n",
    "            print(f'data/batch_{i}/JSON/kaggle_data_{i}.json')\n",
    "            with open(file=f'data/batch_{i}/JSON/kaggle_data_{i}.json') as f:\n",
    "                raw_data = json.load(f)\n",
    "            sub_df = create_data_frame(raw_data, f'data/batch_{i}/background_images')\n",
    "            df = df.append(sub_df)\n",
    "        df.to_csv(path)\n",
    "        df = pd.read_csv(path).drop(columns = 'Unnamed: 0')\n",
    "    else:\n",
    "        df = pd.read_csv(path).drop(columns = 'Unnamed: 0')\n",
    "\n",
    "    list_cols = ['xmins_raw', 'xmaxs_raw', 'ymins_raw', 'ymaxs_raw', 'xmins', 'xmaxs', 'ymins', 'ymaxs']\n",
    "    for c in list_cols:\n",
    "        df[c] = df[c].apply(json.loads)\n",
    "\n",
    "    df['latex'] = df['latex'].replace(\"'\\\\\\\\\", \"'\\\\\")\n",
    "    df['latex'] = df['latex'].apply(ast.literal_eval)\n",
    "    \n",
    "    #vocab = df['latex'].explode().unique().tolist()[0]\n",
    "    df['visible_latex_chars'] = df['visible_latex_chars'].replace(\"'\\\\\\\\\", \"'\\\\\")\n",
    "    df['visible_latex_chars'] = df['visible_latex_chars'].apply(ast.literal_eval)\n",
    "    \n",
    "    with open(file=f'data/extras/visible_char_map.json') as f:\n",
    "        visible_char_map = json.load(f)\n",
    "    \n",
    "    return df, visible_char_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1269ec7-d3b7-4a13-9295-86097c87fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df):\n",
    "    X_train, X_test = train_test_split(df, test_size=0.20, random_state=4995)\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n",
    "def prepare_data(batch_size = 32, caption_task = False):\n",
    "    df, visible_char_map = load_data()\n",
    "\n",
    "    if caption_task:\n",
    "        l = []\n",
    "        for i in df['latex'].tolist():\n",
    "            for j in i:\n",
    "                l.append(j)\n",
    "\n",
    "        classes = sorted(list(set(l)))\n",
    "        num_classes = len(set(l))\n",
    "\n",
    "        visible_char_map = {}\n",
    "        for idx, symbol in enumerate(classes):\n",
    "            visible_char_map[symbol] = idx + 1 \n",
    "\n",
    "        return df, visible_char_map, num_classes, classes\n",
    "        \n",
    "    else:\n",
    "        # num_classes = len(visible_char_map)\n",
    "\n",
    "        l = []\n",
    "        for i in df['visible_latex_chars'].tolist():\n",
    "            for j in i:\n",
    "                l.append(j)\n",
    "\n",
    "        classes = sorted(list(set(l)))\n",
    "        num_classes = len(set(l))\n",
    "\n",
    "        visible_char_map = {}\n",
    "        for idx, symbol in enumerate(classes):\n",
    "            visible_char_map[symbol] = idx + 1 \n",
    "\n",
    "        return df, visible_char_map, num_classes, classes\n",
    "\n",
    "def build_dataloaders(df, visible_char_map, test_set = False, df2 = None,  batch_size = 32, bad_classes = None, caption = False, pad_index = 0):\n",
    "\n",
    "    data_transforms = {\n",
    "      'train': transforms.Compose([\n",
    "          transforms.Resize((896,896)),\n",
    "          transforms.RandomHorizontalFlip(),\n",
    "          transforms.ToTensor(),\n",
    "          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "      ]),\n",
    "      'val': transforms.Compose([\n",
    "          transforms.Resize((896,896)),\n",
    "          #transforms.CenterCrop(256),\n",
    "          #transforms.RandomHorizontalFlip(),\n",
    "          transforms.ToTensor(),\n",
    "          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "      ]),\n",
    "    }\n",
    "    \n",
    "    if caption and test_set:\n",
    "        test_dataset = HandwrittenCaptionDataset(df, visible_char_map, transform = data_transforms['train'], return_file_name = True, train_mode = False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False, num_workers=1, collate_fn=lambda x: pad_batch_rnn_validation_file_name(x, pad_index=pad_index))\n",
    "        return test_loader\n",
    "    \n",
    "    if df2 is None:\n",
    "        train_df, val_df = split_dataframe(df)\n",
    "    else:\n",
    "        train_df, val_df = df, df2\n",
    "    \n",
    "    if caption:\n",
    "        train_dataset = HandwrittenCaptionDataset(train_df, visible_char_map, transform = data_transforms['train'])\n",
    "        train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers=1, collate_fn=lambda x: pad_batch_rnn(x, pad_index=pad_index))\n",
    "\n",
    "        val_dataset = HandwrittenCaptionDataset(val_df, visible_char_map, transform = data_transforms['val'], train_mode = False)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False, num_workers=1, collate_fn=lambda x: pad_batch_rnn_validation(x, pad_index=pad_index))\n",
    "        \n",
    "    else:\n",
    "        train_dataset = HandwrittenDataset(train_df, visible_char_map, transform = data_transforms['train'], bad_classes = bad_classes)\n",
    "        train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers=1)\n",
    "\n",
    "        val_dataset = HandwrittenDataset(val_df, visible_char_map, transform = data_transforms['val'], bad_classes = bad_classes)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False, num_workers=1)\n",
    "    \n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d3160e-530e-47eb-a212-2f6b653410d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, visible_char_map, num_classes, classes = prepare_data(caption_task = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f15fad-5de2-45da-beeb-b507eb50bf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embedding(data, num_classes):\n",
    "    \n",
    "    cores = (\n",
    "            multiprocessing.cpu_count() - 1\n",
    "        )\n",
    "    \n",
    "    #window = 2\n",
    "    #skipgram = True\n",
    "    vectorsize = 150\n",
    "    #iterations = 1\n",
    "\n",
    "    for skipgram in [True, False]:\n",
    "        for window in [2,3,4]:\n",
    "            for vocab_size in [num_classes]:\n",
    "                for iterations in [15, 20]:\n",
    "                    print(skipgram, window,iterations)\n",
    "\n",
    "                    mod_name = f\"skipgram_{str(skipgram)}_window_{str(window)}_iterations_{str(iterations)}_vocabsize_{str(vocab_size)}_\"\n",
    "                    print(mod_name)\n",
    "                    model = gensim.models.Word2Vec(\n",
    "                        data,\n",
    "                        vector_size = vectorsize,\n",
    "                        window=window,\n",
    "                        workers=cores,\n",
    "                        sg=skipgram,\n",
    "                        epochs=iterations,\n",
    "                        sample=0,\n",
    "                    )\n",
    "\n",
    "                    print(model)\n",
    "                    print(model.total_train_time)\n",
    "                    print(model.get_latest_training_loss())\n",
    "\n",
    "                    filename = 'embedding_models/'+ mod_name  + \".model\"\n",
    "                    model.wv.save(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b631031-b9f3-44a5-be6e-d34f8ed7823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df.latex.tolist()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceade433-3e4d-43be-9b33-ff87d1d3b30b",
   "metadata": {},
   "source": [
    "# Train Word embedding without special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b42032a-d6ec-4ee7-b164-e8f168dbcbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding(sentences, num_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
